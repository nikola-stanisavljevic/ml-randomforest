{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hepatitis Dataset Classification in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korišćen dataset <a href=\"http://archive.ics.uci.edu/ml/datasets/Hepatitis?fbclid=IwAR1qzvzODcaRh2dLE5u2bf8LYA-7cronNAUlx2ifJfK8eBEWPqg2zOibXIQ\">Hepatitis UCI MLR</a> sadrži 155 instanci, 19 atributa od kojih prva kolona označava klasu da li će ispitanik preživeti ili ne, na osnovu ostalih podataka iz dataset-a.\n",
    "Citation:\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset \n",
    "dataset = pd.read_csv('hepatitis.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Class</th>\n",
       "      <th>2. AGE</th>\n",
       "      <th>3. SEX</th>\n",
       "      <th>4. STEROID</th>\n",
       "      <th>5. ANTIVIRALS</th>\n",
       "      <th>6. FATIGUE</th>\n",
       "      <th>7. MALAISE</th>\n",
       "      <th>8. ANOREXIA</th>\n",
       "      <th>9. LIVER BIG</th>\n",
       "      <th>10. LIVER FIRM</th>\n",
       "      <th>11. SPLEEN PALPABLE</th>\n",
       "      <th>12. SPIDERS</th>\n",
       "      <th>13. ASCITES</th>\n",
       "      <th>14. VARICES</th>\n",
       "      <th>15. BILIRUBIN</th>\n",
       "      <th>16. ALK PHOSPHATE</th>\n",
       "      <th>17. SGOT</th>\n",
       "      <th>18. ALBUMIN</th>\n",
       "      <th>19. PROTIME</th>\n",
       "      <th>20. HISTOLOGY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>135</td>\n",
       "      <td>42</td>\n",
       "      <td>3.5</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>46</td>\n",
       "      <td>52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>?</td>\n",
       "      <td>200</td>\n",
       "      <td>4.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.60</td>\n",
       "      <td>?</td>\n",
       "      <td>242</td>\n",
       "      <td>3.3</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>126</td>\n",
       "      <td>142</td>\n",
       "      <td>4.3</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>4.1</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>4.1</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.20</td>\n",
       "      <td>100</td>\n",
       "      <td>19</td>\n",
       "      <td>3.1</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1. Class   2. AGE   3. SEX  4. STEROID   5. ANTIVIRALS  6. FATIGUE  \\\n",
       "0           2       30        2           1               2           2   \n",
       "1           2       50        1           1               2           1   \n",
       "2           2       78        1           2               2           1   \n",
       "3           2       31        1           ?               1           2   \n",
       "4           2       34        1           2               2           2   \n",
       "..        ...      ...      ...         ...             ...         ...   \n",
       "150         1       46        1           2               2           1   \n",
       "151         2       44        1           2               2           1   \n",
       "152         2       61        1           1               2           1   \n",
       "153         2       53        2           1               2           1   \n",
       "154         1       43        1           2               2           1   \n",
       "\n",
       "     7. MALAISE  8. ANOREXIA  9. LIVER BIG  10. LIVER FIRM  \\\n",
       "0             2            2             1               2   \n",
       "1             2            2             1               2   \n",
       "2             2            2             2               2   \n",
       "3             2            2             2               2   \n",
       "4             2            2             2               2   \n",
       "..          ...          ...           ...             ...   \n",
       "150           1            1             2               2   \n",
       "151           2            2             2               1   \n",
       "152           1            2             1               1   \n",
       "153           2            2             2               2   \n",
       "154           2            2             2               2   \n",
       "\n",
       "     11. SPLEEN PALPABLE  12. SPIDERS  13. ASCITES  14. VARICES  \\\n",
       "0                      2            2            2            2   \n",
       "1                      2            2            2            2   \n",
       "2                      2            2            2            2   \n",
       "3                      2            2            2            2   \n",
       "4                      2            2            2            2   \n",
       "..                   ...          ...          ...          ...   \n",
       "150                    2            1            1            1   \n",
       "151                    2            2            2            2   \n",
       "152                    2            1            2            2   \n",
       "153                    1            1            2            1   \n",
       "154                    1            1            1            2   \n",
       "\n",
       "     15. BILIRUBIN  16. ALK PHOSPHATE  17. SGOT  18. ALBUMIN  19. PROTIME  \\\n",
       "0             1.00                 85        18          4.0            ?   \n",
       "1             0.90                135        42          3.5            ?   \n",
       "2             0.70                 96        32          4.0            ?   \n",
       "3             0.70                 46        52          4.0           80   \n",
       "4             1.00                  ?       200          4.0            ?   \n",
       "..             ...                ...       ...          ...          ...   \n",
       "150           7.60                  ?       242          3.3           50   \n",
       "151           0.90                126       142          4.3            ?   \n",
       "152           0.80                 75        20          4.1            ?   \n",
       "153           1.50                 81        19          4.1           48   \n",
       "154           1.20                100        19          3.1           42   \n",
       "\n",
       "      20. HISTOLOGY  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "..              ...  \n",
       "150               2  \n",
       "151               2  \n",
       "152               2  \n",
       "153               2  \n",
       "154               2  \n",
       "\n",
       "[155 rows x 20 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pošto dataset nije dobro balansiran, isprobane su dve metode: Upsampling i Undersampling. \n",
    "Undersampling metoda u kojoj se uzimaju 32 slučaja od maksimalno 32 u kojima je ispitanik preminuo, a od 123 preostalih slučajeva kada je ispitanik preživeo uzimaju se samo 32 instance dovela je do lošijih rezultata zbog umanjenog skupa podataka koji obrađuje algoritam. Novi dataset u ovom slučaju ima 64 umesto originalnih 155 instanci, a preciznost na kraju obrade iznosila je između 68,75% i 81,25%.\n",
    "Za isprobanu metodu Upsampling, rezultat preciznosti predikcije u procentima iznosio je 90,32%, a kod je prikazan u nastavku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "died = dataset[dataset['1. Class'] == 1] \n",
    "lived = dataset[dataset['1. Class'] == 2]\n",
    "non_normalized_dataset = dataset\n",
    "from sklearn.utils import resample\n",
    "died_upsampled = resample(died, replace=True,  n_samples=123, random_state=10)\n",
    "dataset = pd.concat([died_upsampled, lived])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "featureMatrix_Independent predstavlja podatke izdvojene iz dataset-a koji se odnose na nezavisne atribute\n",
    "\n",
    "variableVector_Dependent predstavlja podatke iz prve kolone dataset-a koji označavaju da li je ispitanik preživeo ili ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureMatrix_Independent = dataset.iloc[:, 1:20].values\n",
    "variableVector_Dependent = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U nastavku se obrađuje dataset kako bi se uklonili podaci koji nedostaju. Pokazalo se da je od izbora strategije mean, most_frequent i median - most_frequent najbolji izbor. Obrađuje se samo matrica nezavisnih podataka i to kolone u kojima postoje '?' umesto konkretnog podatka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = '?', strategy = 'most_frequent')\n",
    "imputer.fit(featureMatrix_Independent[:, 2:20])\n",
    "featureMatrix_Independent[:, 2:20] = imputer.transform(featureMatrix_Independent[:, 2:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U narednom delu deklarišu se i inicijalizuju promenljive F_train, D_train (trening podaci nezavisnih i zavisne promenljive) i F_test i D_test (podaci za testiranje nezavisnih i zavisne promenljive), uzima se 25% dataset-a za skup podataka za testiranje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Test Set and Training Set from Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "F_train, F_test, D_train, D_test = train_test_split(featureMatrix_Independent, variableVector_Dependent, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao što smo mogli primetiti u dataset-u atributi su prikazani vrednostima u različitim opsezima. Ovo može uticati na algoritam i određeni atributi sa velikim vrednostima mogu postati dominantni. Zbog toga je neophodno odraditi skaliranje, upotrebom StandardScaler-a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for independent features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_F = StandardScaler()\n",
    "F_train = scale_F.fit_transform(F_train)\n",
    "F_test = scale_F.transform(F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35752098, -0.23973165,  1.14017543, ..., -1.34742913,\n",
       "        -0.66571715,  0.76564149],\n",
       "       [ 0.35752098, -0.23973165,  1.14017543, ...,  0.88363462,\n",
       "         1.07281426,  0.76564149],\n",
       "       [ 1.33814997, -0.23973165, -0.87705802, ...,  0.04698572,\n",
       "        -0.31801087,  0.76564149],\n",
       "       ...,\n",
       "       [-0.78654617, -0.23973165, -0.87705802, ..., -0.78966319,\n",
       "        -0.66571715,  0.76564149],\n",
       "       [ 0.35752098, -0.23973165,  1.14017543, ...,  0.88363462,\n",
       "         1.07281426,  0.76564149],\n",
       "       [-1.60373699, -0.23973165,  1.14017543, ...,  1.44140056,\n",
       "        -0.66571715, -1.30609431]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U nastavku se koristi ExtraTreesClassifier kako bismo odredili važnost udela atributa u analizi i predikciji.\n",
    "U rezultatu vidimo da su prva dva atributa (18 i 5) histology i malaise dominantni pri predikciji, ali i neki od narednih mogu doprineti treniranju i predikciji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 18 (0.114499)\n",
      "2. feature 5 (0.109938)\n",
      "3. feature 10 (0.082404)\n",
      "4. feature 13 (0.080934)\n",
      "5. feature 11 (0.073410)\n",
      "6. feature 16 (0.061029)\n",
      "7. feature 17 (0.052287)\n",
      "8. feature 0 (0.050669)\n",
      "9. feature 12 (0.049746)\n",
      "10. feature 14 (0.049134)\n",
      "11. feature 15 (0.043197)\n",
      "12. feature 9 (0.043044)\n",
      "13. feature 4 (0.041587)\n",
      "14. feature 8 (0.032603)\n",
      "15. feature 7 (0.029336)\n",
      "16. feature 2 (0.027486)\n",
      "17. feature 6 (0.025797)\n",
      "18. feature 1 (0.022969)\n",
      "19. feature 3 (0.009930)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(featureMatrix_Independent, variableVector_Dependent)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(featureMatrix_Independent.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(featureMatrix_Independent.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(featureMatrix_Independent.shape[1]), indices)\n",
    "plt.xlim([-1, featureMatrix_Independent.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal component analysis) je jedna od metoda za smanjivanje dimenzionalnosti prostora. Ona koristi Singular Value Decomposition i linearno smanjuje dimenzionalnost. Za vrednosti n_components 8 i 10 donosi preciznost od 87,09% i 90,32% respektivno. Promenljiva explained_variance_ratio prikazuje nivo vrednosti atributa koji su selektovani od strane algoritma za svaku od n_components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2393288 , 0.12726574, 0.09998906, 0.08685933, 0.07231423,\n",
       "       0.05780434, 0.05370694, 0.04443778, 0.04312351, 0.03382391])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying PCA (feature extraction)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10) \n",
    "F_train = pca.fit_transform(F_train)\n",
    "F_test = pca.fit_transform(F_test)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classification koristi noviju verziju algoritma koji se bazira na stablima odlučivanja. \n",
    "n_estimators parametar određuje broj stabala koji se koristi u analizi, a kriterijum može biti baziran na gini indeksu ili entropiji. Rezultati za različite kombinacije su sledeći:\n",
    "n_estimators = 10, criterion = 'entropy' => 90,32%\n",
    "n_estimators = 10, criterion = 'gini' => 90,32%\n",
    "n_estimators = 12, criterion = 'gini' => 87,09%\n",
    "n_estimators = 12, criterion = 'entropy' => 91,93%\n",
    "Za oba kriterijuma ukoliko imamo više od 30 stabala preciznost predikcije se smanjuje i iznosi oko 88%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=12,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Random Forest Classification to the Training Set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 12, criterion = 'entropy', max_features = 'auto', random_state = 0)\n",
    "classifier.fit(F_train, D_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U nastavku se vrši predikcija skupa za testiranje podataka na osnovu istreniranog algoritma Random Forest klasifikatora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "prediction = classifier.predict(F_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrica konfuzije predstavlja rezultate predikcije na test podacima. Na glavnoj dijagonali matrice nalaze se pogotci, dok su na sporednoj promašaji, a na osnovu ovih brojeva dobija se accuracy_score prikazan ispod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conMat = confusion_matrix(D_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  2],\n",
       "       [ 3, 30]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ac=accuracy_score(D_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9193548387096774"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preciznost algoritma je 91,93%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
